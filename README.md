# RAGCore‚ÄëX

> **A modular Retrieval‚ÄëAugmented Generation (RAG) core service that exposes high‚Äëlevel Web APIs for document ingest, semantic search, comparison, and question‚Äëanswering‚Äîbuilt to power multiple downstream products such as ScamShield‚ÄëAI, Relulens‚ÄëAI, and InsightDoc‚ÄëAI.**
>
> *LoRA or other parameter‚Äëefficient fine‚Äëtuning is **not** enabled yet, but optional support is on the roadmap once the local‚Äëmodel pipeline is solid.*

---

## Authors

* [Jerry Hung](https://github.com/JerryHung1030)
* [Ken Su](https://github.com/ken22i)
* [SJ](https://github.com/shih1999)

---

## ‚ú® Features

* **End‚Äëto‚Äëend RAG pipeline** powered by OpenAI GPT‚Äë4o (default) or a local Llama adapter.
* **Hierarchical JSON ingestion** (`level1` ‚Üí `level5`) with automatic flattening, optional chunk‚Äësplitting, and schema validation.
* **Pluggable embeddings** via `langchain-openai` (defaults to `text‚Äëembedding‚Äëada‚Äë002`).
* **Vector store abstraction** built on Qdrant, supporting upsert, search with metadata filters, and async operations.
* **PromptBuilder** that fits the entire query + candidates within a configurable token budget and gracefully backs off.
* **ResultFormatter** that parses the LLM JSON output, merges similarity scores, filters by confidence, and normalizes direction (forward / reverse / both).
* **Job orchestration** with Redis‚ÄëRQ (batch ingest + RAG jobs) and a minimal FastAPI fa√ßade.
* **Rich logging** via loguru with daily rotation.
* **Extensive unit tests** (PyTest) with mocks for fast, cost‚Äëfree CI.

---

## Table of Contents

1. [‚ú® Features](#features)
2. [üó∫Ô∏è Architecture Overview](#architecture-overview)
3. [üìÇ Project Structure](#project-structure)
4. [üöÄ Quick Start](#quick-start)
5. [‚öôÔ∏è Configuration](#configuration)
6. [üõ†Ô∏è CLI & API Usage](#cli--api-usage)
7. [üß™ Testing](#testing)
8. [ü§ù Contributing](#contributing)
9. [üìÑ License](#license)

---

## ‚ú® Features

* **End‚Äëto‚Äëend RAG pipeline** powered by **OpenAI GPT‚Äë4o** (default) or **local Llama** adapter.
* **Hierarchical JSON ingestion** (`level1 ‚Üí level5`) with automatic flattening, optional chunk‚Äësplitting, and schema validation.
* **Pluggable embeddings** via `langchain-openai` (defaults to `text‚Äëembedding‚Äëada‚Äë002`).
* **Vector store abstraction** built on **Qdrant**, supporting upsert, search w/ metadata filters, and async operations.
* **PromptBuilder** that fits the entire query + candidates within a configurable token budget and gracefully backs off.
* **ResultFormatter** that parses the LLM JSON output, merges similarity scores, filters by confidence, and normalizes direction (forward / reverse / both).
* **Job orchestration** with **Redis‚ÄëRQ** (batch ingest + RAG jobs) and a minimal **FastAPI** facade.
* **Rich logging** via **loguru** with daily rotation.
* **Extensive unit tests** (PyTest) with mocks for fast, cost‚Äëfree CI.

---

## üó∫Ô∏è Architecture Overview

```text
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Input   ‚îÇ     ‚îÇ   Embedding   ‚îÇ     ‚îÇ  Qdrant  ‚îÇ
‚îÇ JSON(s)  ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÄ‚îÄ‚îÇ  Manager      ‚îÇ‚îÄ‚îÄ‚ñ∫‚îÄ‚îÄ‚îÇ Vector DB‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                                   ‚ñ≤
      ‚ñº                                   ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  search(top‚Äëk)  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAGEngine ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫‚îÇ PromptBuilder   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                 ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ   LLM request / stream          ‚îÇprompt
      ‚ñº                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLMProxy  ‚îÇ‚óÑ‚îÄ‚îÄ GPT‚Äë4o / Llama ‚îÇ ResultFormat ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                                   ‚îÇ
      ‚ñº                                   ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAG Result  ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÇ    Client    ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## üìÇ Project Structure

```text
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ rag_core/            # Core RAG logic (domain, application, infrastructure)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ domain/          # Pydantic schemas, validation, exceptions
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ application/     # PromptBuilder, RAGEngine, ResultFormatter
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ infrastructure/  # Embeddings, VectorStore, LLM adapters
‚îÇ   ‚îú‚îÄ‚îÄ interfaces/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cli_main.py      # One‚Äëshot CLI pipeline
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ jobs/            # RQ Job runner
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ api/             # FastAPI facade
‚îÇ   ‚îú‚îÄ‚îÄ data/                # Example hierarchical JSON datasets
‚îÇ   ‚îî‚îÄ‚îÄ config/              # YAML + pydantic‚Äësettings
‚îú‚îÄ‚îÄ tests/                   # PyTest suites with extensive mocks
‚îú‚îÄ‚îÄ requirements.txt         # Python dependencies
‚îî‚îÄ‚îÄ README.md
```

---

## üöÄ Quick Start

### 1. Prerequisites

* **Python ‚â• 3.12**
* **Docker** (for Qdrant & Redis) *or* native installations
* An **OpenAI API key** (required for GPT-4o)

### 2. Environment Setup

```bash
# set OpenAI API Key
$ export OPENAI_API_KEY="your-api-key-here"

# Or, you can create a .env file
$ echo "OPENAI_API_KEY=your-api-key-here" > .env
```

> ‚ö†Ô∏è **Important Note**: Make sure to set up your API Key before running any commands, otherwise the program will not function properly.

### 3. Clone & Install

```bash
# 1‚É£  Clone
$ git clone https://github.com/your-org/RAGCore-X.git
$ cd RAGCore-X

# 2‚É£  Create venv
$ python -m venv .venv && source .venv/bin/activate

# 3‚É£  Install deps
$ pip install -r requirements.txt
```

### 4. Spin up services

```bash
# Qdrant
$ docker run -d --name qdrant -p 6333:6333 qdrant/qdrant

# Redis (for RQ)
$ docker run -d --name redis -p 6379:6379 redis:7
```

### 5. Run the **demo CLI**

```bash
$ python -m src.interfaces.cli_main
```

This will ingest the sample `scam_input.json` & `scam_references.json`, execute a reverse‚Äëdirection RAG, and write results to `src/interfaces/output/rag_result.json`.

### 6. Launch the **FastAPI** endpoint (optional)

```bash
$ uvicorn src.interfaces.api.fastapi_app:app --reload --port 8000
```

---

## ‚öôÔ∏è Configuration

ScamShield-AI uses a layered configuration system that combines environment variables, YAML files, and Pydantic settings for maximum flexibility.

### Environment Variables

The following environment variables can be used to override default settings:

| Variable | Description | Default | Required |
|----------|-------------|---------|----------|
| `OPENAI_API_KEY` | Your OpenAI API key | - | Yes |
| `LLM_MODEL` | LLM model to use | `gpt-4o` | No |
| `QDRANT_URL` | Qdrant server URL | `http://localhost:6333` | No |
| `QDRANT_COLLECTION` | Qdrant collection name | `dev_rag_collection` | No |
| `IS_DEBUG` | Enable debug mode | `false` | No |

### Configuration Files

The system uses two types of configuration files:

1. **Base Configuration** (`src/config/settings.base.yml`):
   - Contains default settings for all environments
   - Version controlled
   - Should not contain sensitive information

2. **Local Configuration** (`settings.local.yml`):
   - Overrides base settings for local development
   - Not version controlled
   - Can contain environment-specific settings

Example base configuration:

```yaml
system:
  is_debug: true
  log_dir: logs
  log_file_path: app.log

llm:
  model: gpt-4o
  temperature: 0.7
  max_tokens: 4096
  max_prompt_tokens: 8000

vector_db:
  url: http://localhost:6333
  collection: dev_rag_collection
  vector_size: 1536

scenario:
  direction: reverse        # forward / reverse / both
  rag_k_forward: 5
  rag_k_reverse: 20
  cof_threshold: 0.6
  reference_json: "src/data/scam_references.json"
  input_json: "src/data/scam_input.json"
```

### Configuration Priority

Settings are applied in the following order (highest to lowest priority):

1. Environment variables
2. Local configuration file (`settings.local.yml`)
3. Base configuration file (`settings.base.yml`)
4. Default values in code

> **Security Note**: Always store sensitive information (API keys, credentials) in environment variables or `.env` files, never in configuration files.

---

## üõ†Ô∏è CLI & API Usage

### CLI

```bash
# Forward direction (input ‚Üí reference)
$ python -m src.interfaces.cli_main --run-id run_1 \
      --direction forward
```

### API Usage

RAGCore-X exposes a FastAPI-based web API for submitting RAG jobs, checking their status, and retrieving results. The API is designed for asynchronous, scalable document analysis and comparison workflows.

#### Start the API Server

Make sure Redis is running, then launch the API service:

```bash
python src/interfaces/run_api.py
```

The API will be available at `http://localhost:8000` by default.

#### Main Endpoints

- **POST `/api/v1/rag`** ‚Äî Submit a new RAG job
- **GET `/api/v1/rag/{job_id}/status`** ‚Äî Check job status
- **GET `/api/v1/rag/{job_id}/result`** ‚Äî Retrieve job result
- **GET `/api/v1/rag`** ‚Äî List all jobs (optionally filter by project)
- **DELETE `/api/v1/rag/{job_id}`** ‚Äî Delete a job



#### Notes
For detailed API specifications, please refer to [RAGCore-X_api.xlsx](docs/RAGCore-X_api.xlsx).

### Programmatic (FastAPI + RQ)

```python
from src.interfaces.api.fastapi_app import start_rag_job, RAGJobRunner
from src.rag_core.infrastructure import setup_core

embed_mgr, vec_index, llm_mgr = setup_core()
runner = RAGJobRunner(vec_index, RAGEngine(embed_mgr, vec_index, llm_mgr))

job_id = start_rag_job(
    job_runner=runner,
    project_id="demo_proj",
    scenario={"direction": "reverse", "rag_k_reverse": 20},
    input_json_path="/path/input.json",
    reference_json_path="/path/ref.json",
    callback_url="https://your.backend/api/rag_callback"
)
print("Enqueued RAG job:", job_id)
```
---

## üß™ Testing

```bash
$ pytest -q
```

The test suite mocks **OpenAI**, **Qdrant**, and **Embeddings** to provide fast, deterministic results (< 5 s on a laptop).

---

## üöß Development Status & Roadmap

> **Status ‚Äî Alpha preview.** The fundamental RAG loop is stable, but the orchestration layer is being redesigned.

### ‚úÖ Completed so far

* Core `rag_core` engine (embedding ‚Üí search ‚Üí prompt ‚Üí LLM ‚Üí result) proven via CLI demo.
* Hierarchical JSON ingestion, schema validation, and optional chunk‚Äësplitting.
* Qdrant vector store adapter with async helpers.
* OpenAI GPT‚Äë4o & local Llama adapters.
* Initial Redis‚ÄëRQ + FastAPI proof‚Äëof‚Äëconcept.
* CI‚Äëready PyTest suite with heavy mocking.

### üõ†Ô∏è In progress

* **Orchestration decoupling** ‚Äì Redis/RQ dependencies will be extracted from `rag_core`; the job queue becomes an *optional* outer service.
* **Packaging** ‚Äì publish `rag_core` as a standalone `pip install scamshield-rag` so any backend can import and wire up its own queue.
* **Scenario templates** ‚Äì ship more scoring rules, prompt recipes, and evaluation scripts.

### üóìÔ∏è Planned / help wanted

* Reference implementation of a lightweight `FastAPI` + `StarterQueue` repo demonstrating how to plug in your own Redis, Sidekiq, or Celery workers.
* Async streaming helpers (WebSocket / SSE) for real‚Äëtime UIs.
* Multilingual (JP/KR/EN) scam datasets & regulatory corpora.
* Memory‚Äëaware chunking + performance benchmarks.
* **Local fine‚Äëtuning workflow** ‚Äî LoRA / QLoRA recipes for Llama‚Äë3 or Phi‚Äë3 to run completely offline.

---

## ü§ù Contributing

1. Fork ‚Üí feature branch ‚Üí Pull Request.
2. Make sure `pytest` passes and `pre‚Äëcommit run --all-files` shows no lint errors.
3. Add / update docs where applicable.

We welcome new **datasets**, **scenario templates**, and **LLM adapters**!

---

## üìÑ License

Proprietary Software License Agreement

Copyright (c) 2024 Institute for Information Industry (III), Cyber Security Technology Institute (CSTI)

All rights reserved. This software is proprietary and confidential. Unauthorized copying, modification, distribution, or use is strictly prohibited.

> ¬© 2024 Institute for Information Industry (III), Cyber Security Technology Institute (CSTI). Built with ‚ù§Ô∏è in Taiwan.

